{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f152ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from uuid import uuid4\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b8a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY \"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = os.getenv(\"LANGSMITH_TRACING\")\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = os.getenv(\"LANGSMITH_ENDPOINT\")\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = os.getenv(\"LANGSMITH_PROJECT\")\n",
    "os.environ[\"QDRANT_URL\"] = os.getenv(\"QDRANT_URL\")\n",
    "os.environ[\"QDRANT_API_KEY\"] = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e98048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# df =pd.read_csv(\"data/conversation_data.csv\")\n",
    "# df.drop(\"ID\", axis=1, inplace=True)\n",
    "\n",
    "# df.to_csv(\"data/no_id_conv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f941608",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(\n",
    "    file_path= \"data/no_id_conv.csv\",\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",\n",
    "\n",
    "    },\n",
    " \n",
    ")\n",
    "\n",
    "\n",
    "docs=  loader.load()\n",
    "docs[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2140c0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=300)\n",
    "texts = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d153fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5604ba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "# QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "\n",
    "\n",
    "# qdrant = QdrantVectorStore.from_documents(\n",
    "#     docs,\n",
    "#     hf,\n",
    "#     url=QDRANT_URL,\n",
    "#     prefer_grpc=False,\n",
    "#     force_recreate= True,\n",
    "#     api_key=QDRANT_API_KEY,\n",
    "#     collection_name=\"awadoc_demo\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9818b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qdrant = QdrantVectorStore.from_existing_collection(\n",
    "#     embedding=embeddings,\n",
    "#     collection_name=\"my_documents\",\n",
    "#     url=\"http://localhost:6333\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc57ea62",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, hf,persist_directory=\"data/\")\n",
    "retriever =db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":4})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c5033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,)\n",
    "\n",
    "template = \"\"\"You are a professional and empathetic medical assistant designed to understand and respond to patients based on previous doctor-patient conversations.\n",
    "\n",
    "You are given context in the form of a past conversation between a patient and a doctor. Use this context to guide your understanding and responses. \n",
    "Answer the question based on the cotext and ChatHistory. Especially take the latest question into consideration.\n",
    "Your responsibilities:\n",
    "\n",
    "1. Carefully read the provided conversation between the patient and doctor.\n",
    "2. Use the dialogue to understand the patient's symptoms, history, and concerns.\n",
    "3. When the patient's input is unclear or incomplete, ask follow-up questions to gain full clarity before responding.\n",
    "4. Respond like a human doctor would â€” with clarity, compassion, and professionalism.\n",
    "5. Do NOT give final diagnoses. Instead, suggest possibilities, give helpful information, and recommend next steps.\n",
    "6. If the conversation lacks enough information, politely ask the patient to clarify or provide more details.\n",
    "7. Do not hallucinate or make up facts. Stick to the context or ask the right questions to gather more information.\n",
    "8. Keep your language friendly, clear, and medically sound.\n",
    "9. If from user messages, he requires consultation just tell the user\n",
    "\n",
    "(a conversation between a patient and doctor):\n",
    "\n",
    "ChatHistory: {history}\n",
    "Context: {context}\n",
    "\n",
    "Begin the conversation based on this context.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = prompt|llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48546120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.schema import Document\n",
    "from typing import List, TypedDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5964a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: List[BaseMessage]\n",
    "    documents: List[Document]\n",
    "    on_topic: str\n",
    "    rephrased_question: str\n",
    "    proceed_to_generate: bool\n",
    "    rephrase_count: int\n",
    "    question: HumanMessage\n",
    "\n",
    "\n",
    "class GradeQuestion(BaseModel):\n",
    "    score: str = Field(\n",
    "        description=\"Question is about the specified topics? If yes -> 'Yes' if not -> 'No'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def question_rewriter(state: AgentState):\n",
    "    print(f\"Entering question_rewriter with following state: {state}\")\n",
    "\n",
    "    state[\"documents\"] = []\n",
    "    state[\"on_topic\"] = \"\"\n",
    "    state[\"rephrased_question\"] = \"\"\n",
    "    state[\"proceed_to_generate\"] = False\n",
    "    state[\"rephrase_count\"] = 0\n",
    "\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "\n",
    "    if state[\"question\"] not in state[\"messages\"]:\n",
    "        state[\"messages\"].append(state[\"question\"])\n",
    "\n",
    "    if len(state[\"messages\"]) > 1:\n",
    "        conversation = state[\"messages\"][:-1]\n",
    "        current_question = state[\"question\"].content\n",
    "        messages = [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful assistant that rephrases the user's question to be a standalone question optimized for retrieval.\"\n",
    "            )\n",
    "        ]\n",
    "        messages.extend(conversation)\n",
    "        messages.append(HumanMessage(content=current_question))\n",
    "        rephrase_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "        \n",
    "        prompt = rephrase_prompt.format()\n",
    "        response = llm.invoke(prompt)\n",
    "        better_question = response.content.strip()\n",
    "        print(f\"question_rewriter: Rephrased question: {better_question}\")\n",
    "        state[\"rephrased_question\"] = better_question\n",
    "    else:\n",
    "        state[\"rephrased_question\"] = state[\"question\"].content\n",
    "    return state\n",
    "\n",
    "def question_classifier(state: AgentState):\n",
    "    print(\"Entering question_classifier\")\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\" You are a classifier that determines whether a user question is medical-related.\n",
    "\n",
    "Instructions:\n",
    "- If the question is about medicine, health, symptoms, diseases, treatments, drugs, anatomy, medical advice, or any related medical topic, respond with: Yes\n",
    "- If it is not about a medical or health-related topic, respond with: No\n",
    "\n",
    "Only respond with one word: Yes or No.\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    human_message = HumanMessage(\n",
    "        content=f\"User question: {state['rephrased_question']}\"\n",
    "    )\n",
    "    grade_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "  \n",
    "    structured_llm = llm.with_structured_output(GradeQuestion)\n",
    "    grader_llm = grade_prompt | structured_llm\n",
    "    result = grader_llm.invoke({})\n",
    "    state[\"on_topic\"] = result.score.strip()\n",
    "    print(f\"question_classifier: on_topic = {state['on_topic']}\")\n",
    "    return state\n",
    "\n",
    "def on_topic_router(state: AgentState):\n",
    "    print(\"Entering on_topic_router\")\n",
    "    on_topic = state.get(\"on_topic\", \"\").strip().lower()\n",
    "    if on_topic == \"yes\":\n",
    "        print(\"Routing to retrieve\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(\"Routing to off_topic_response\")\n",
    "        return \"off_topic_response\"\n",
    "\n",
    "\n",
    "def retrieve(state: AgentState):\n",
    "    print(\"Entering retrieve\")\n",
    "    documents = retriever.invoke(state[\"rephrased_question\"])\n",
    "    print(f\"retrieve: Retrieved {len(documents)} documents\")\n",
    "    state[\"documents\"] = documents\n",
    "    return state\n",
    "\n",
    "\n",
    "class GradeDocument(BaseModel):\n",
    "    score: str = Field(\n",
    "        description=\"Document is relevant to the question? If yes -> 'Yes' if not -> 'No'\"\n",
    "    )\n",
    "\n",
    "def retrieval_grader(state: AgentState):\n",
    "    print(\"Entering retrieval_grader\")\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a grader assessing the relevance of a retrieved document to a user question.\n",
    "Only answer with 'Yes' or 'No'.\n",
    "\n",
    "If the document contains information relevant to the user's question, respond with 'Yes'.\n",
    "Otherwise, respond with 'No'.\"\"\"\n",
    "    )\n",
    "\n",
    "    structured_llm = llm.with_structured_output(GradeDocument)\n",
    "\n",
    "    relevant_docs = []\n",
    "    for doc in state[\"documents\"]:\n",
    "        human_message = HumanMessage(\n",
    "            content=f\"User question: {state['rephrased_question']}\\n\\nRetrieved document:\\n{doc.page_content}\"\n",
    "        )\n",
    "        grade_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "        grader_llm = grade_prompt | structured_llm\n",
    "        result = grader_llm.invoke({})\n",
    "        print(\n",
    "            f\"Grading document: {doc.page_content[:30]}... Result: {result.score.strip()}\"\n",
    "        )\n",
    "        if result.score.strip().lower() == \"yes\":\n",
    "            relevant_docs.append(doc)\n",
    "    state[\"documents\"] = relevant_docs\n",
    "    state[\"proceed_to_generate\"] = len(relevant_docs) > 0\n",
    "    print(f\"retrieval_grader: proceed_to_generate = {state['proceed_to_generate']}\")\n",
    "    return state\n",
    "\n",
    "def proceed_router(state: AgentState):\n",
    "    print(\"Entering proceed_router\")\n",
    "    rephrase_count = state.get(\"rephrase_count\", 0)\n",
    "    if state.get(\"proceed_to_generate\", False):\n",
    "        print(\"Routing to generate_answer\")\n",
    "        return \"generate_answer\"\n",
    "    elif rephrase_count >= 2:\n",
    "        print(\"Maximum rephrase attempts reached. Cannot find relevant documents.\")\n",
    "        return \"cannot_answer\"\n",
    "    else:\n",
    "        print(\"Routing to refine_question\")\n",
    "        return \"refine_question\"\n",
    "    \n",
    "def refine_question(state: AgentState):\n",
    "    print(\"Entering refine_question\")\n",
    "    rephrase_count = state.get(\"rephrase_count\", 0)\n",
    "    if rephrase_count >= 2:\n",
    "        print(\"Maximum rephrase attempts reached\")\n",
    "        return state\n",
    "    question_to_refine = state[\"rephrased_question\"]\n",
    "    system_message = SystemMessage(\n",
    "        content=\"\"\"You are a helpful assistant that slightly refines the user's question to improve retrieval results.\n",
    "Provide a slightly adjusted version of the question.\"\"\"\n",
    "    )\n",
    "    human_message = HumanMessage(\n",
    "        content=f\"Original question: {question_to_refine}\\n\\nProvide a slightly refined question.\"\n",
    "    )\n",
    "    refine_prompt = ChatPromptTemplate.from_messages([system_message, human_message])\n",
    "    \n",
    "    prompt = refine_prompt.format()\n",
    "    response = llm.invoke(prompt)\n",
    "    refined_question = response.content.strip()\n",
    "    print(f\"refine_question: Refined question: {refined_question}\")\n",
    "    state[\"rephrased_question\"] = refined_question\n",
    "    state[\"rephrase_count\"] = rephrase_count + 1\n",
    "    return state\n",
    "\n",
    "def generate_answer(state: AgentState):\n",
    "    print(\"Entering generate_answer\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        raise ValueError(\"State must include 'messages' before generating an answer.\")\n",
    "\n",
    "    history = state[\"messages\"]\n",
    "    documents = state[\"documents\"]\n",
    "    rephrased_question = state[\"rephrased_question\"]\n",
    "\n",
    "    response = rag_chain.invoke(\n",
    "        {\"history\": history, \"context\": documents, \"question\": rephrased_question}\n",
    "    )\n",
    "\n",
    "    generation = response.content.strip()\n",
    "\n",
    "    state[\"messages\"].append(AIMessage(content=generation))\n",
    "    print(f\"generate_answer: Generated response: {generation}\")\n",
    "    return state\n",
    "\n",
    "def cannot_answer(state: AgentState):\n",
    "    print(\"Entering cannot_answer\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "    state[\"messages\"].append(\n",
    "        AIMessage(\n",
    "            content=\"You are currently been moved to a doctor for proper medical attention.\"\n",
    "        )\n",
    "    )\n",
    "    return state\n",
    "\n",
    "\n",
    "def off_topic_response(state: AgentState):\n",
    "    print(\"Entering off_topic_response\")\n",
    "    if \"messages\" not in state or state[\"messages\"] is None:\n",
    "        state[\"messages\"] = []\n",
    "    state[\"messages\"].append(AIMessage(content=\"I'm sorry! That question is not related to what I have been trained for\"))\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0b956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "checkpointer = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7854d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d016f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"question_rewriter\", question_rewriter)\n",
    "workflow.add_node(\"question_classifier\", question_classifier)\n",
    "workflow.add_node(\"off_topic_response\", off_topic_response)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"retrieval_grader\", retrieval_grader)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "workflow.add_node(\"refine_question\", refine_question)\n",
    "workflow.add_node(\"cannot_answer\", cannot_answer)\n",
    "\n",
    "workflow.add_edge(\"question_rewriter\", \"question_classifier\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"question_classifier\",\n",
    "    on_topic_router,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"off_topic_response\": \"off_topic_response\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"retrieval_grader\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieval_grader\",\n",
    "    proceed_router,\n",
    "    {\n",
    "        \"generate_answer\": \"generate_answer\",\n",
    "        \"refine_question\": \"refine_question\",\n",
    "        \"cannot_answer\": \"cannot_answer\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"refine_question\", \"retrieve\")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "workflow.add_edge(\"cannot_answer\", END)\n",
    "workflow.add_edge(\"off_topic_response\", END)\n",
    "workflow.set_entry_point(\"question_rewriter\")\n",
    "graph = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ec653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "# from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "# display(\n",
    "#     Image(\n",
    "#         graph.get_graph().draw_mermaid_png(\n",
    "#             draw_method=MermaidDrawMethod.API,\n",
    "#         )\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a814c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\"question\": HumanMessage(content=\"What does the company Apple do?\")}\n",
    "graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f27740",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    \"question\": HumanMessage(\n",
    "        content=\"What AM I HAVING HEADACHE\"\n",
    "    )\n",
    "}\n",
    "graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 2}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    \"question\": HumanMessage(content=\"I FEEL LIKE DYING\")\n",
    "}\n",
    "graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 3}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8d2676",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = {\"question\": HumanMessage(content=\"When did he start it?\")}\n",
    "graph.invoke(input=input_data, config={\"configurable\": {\"thread_id\": 3}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355f72f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
